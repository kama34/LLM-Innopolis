{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73JHBwym-IfY"
   },
   "source": [
    "## Week 1 : Large Language Models\n",
    "```\n",
    "- Generative Artificial Intelligence (Fall semester 2023)\n",
    "- Professor: Muhammad Fahim\n",
    "- Teaching Assistant: Ahmad Taha\n",
    "```\n",
    "<hr>\n",
    "\n",
    "## Contents\n",
    "```\n",
    "1. Transformers (Implementing a transformer)\n",
    "2. Self-Attention\n",
    "3. Multi-headed attention\n",
    "4. Positional Encoding\n",
    "\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivBBW705o6JG"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) -- Original paper on attention\n",
    "\n",
    "![](http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkzgoQHBzCsS",
    "outputId": "cd5cb229-e10e-45c2-e63e-3415202244eb",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:38:02.347057Z",
     "start_time": "2024-06-05T19:38:00.769471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqbWEqEG04nj"
   },
   "source": [
    "### Transformer Encoder with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqvnRd4zASjV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b03123ca-fe55-4e72-ea9e-8a418db709d0"
   },
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=32)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmaOP-5BAiVG",
    "outputId": "6186f544-8b78-4bdb-d9bb-647a97e0ac78",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:38:49.257031Z",
     "start_time": "2024-06-05T19:38:49.253200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETf4hBGZBmB6"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder contains a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. <br>\n",
    "**The main goal is to efficiently encode the data**\n",
    "\n",
    "![](http://jalammar.github.io/images/t/encoder_with_tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MQv6sV8CHcb"
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "**Keep in mind : The main goal is to encode the data in a much more efficient way** In other words is to create meaningful embeddings<br>\n",
    "- As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
    "\n",
    "\n",
    "**How does Self-Attention work?**\n",
    "\n",
    "Steps:\n",
    "1. For each word, we create a **`Query`** vector, a **`Key`** vector, and a **`Value`** vector.\n",
    "  - What are the **`Query`** vector, a **`Key`** vector, and a **`Value`** vector? : They’re abstractions that are useful for calculating attention... They are a breakdown of the word embeddings\n",
    "2. Calculating self-attention score from **`Query`** **`Key`** vector.\n",
    "3. Divide the scores by 8 (This leads to having more stable gradients)\n",
    "4. Pass the result through a softmax operation (softmax score determines how much each word will be expressed at this position)\n",
    "5. Multiply each value vector by the softmax score\n",
    "6. Sum up the weighted value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RevROKUGFOX"
   },
   "source": [
    "### Step 1\n",
    "\n",
    "For each word, we create a **`Query`** vector, a **`Key`** vector, and a **`Value`** vector.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YDahLOTBDVDP",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:44:17.647842Z",
     "start_time": "2024-06-05T19:44:17.622337Z"
    }
   },
   "outputs": [],
   "source": [
    "# simple sequence = I am here today\n",
    "simple_sequence_embedding = torch.rand((4, 512))\n",
    "\n",
    "# Create weight matrices\n",
    "Wq = torch.normal(0,0.5, (512, 7))\n",
    "Wk = torch.normal(0,0.1, (512, 7))\n",
    "Wv = torch.normal(0,0.2, (512, 7))\n",
    "\n",
    "# Create key, query and value for each word in the senetence\n",
    "queries = simple_sequence_embedding.mm(Wq) # self.q(embedding[0])\n",
    "keys = simple_sequence_embedding.mm(Wk)\n",
    "values = simple_sequence_embedding.mm(Wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2dceawSN2PQ",
    "outputId": "77229293-d009-465f-ec00-f5c0bc9d86ec",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:44:38.443238Z",
     "start_time": "2024-06-05T19:44:38.438817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.5932, -3.2220, -0.3001,  3.4596,  2.8656, -0.8417, -4.0470],\n        [ 0.7683, -1.2544,  2.7111,  0.9582,  5.6401, -1.1286, -5.0858],\n        [ 2.8754, -0.2318,  1.8981,  2.3130,  2.9996,  0.0587, -1.1642],\n        [-0.1914, -1.6522,  2.2397,  2.1431,  5.2016, -1.6490, -3.9122]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhXQl2p8Nzp_",
    "outputId": "a916096a-f073-4938-d3dd-e9d541e52a7c",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:44:41.200612Z",
     "start_time": "2024-06-05T19:44:41.196990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0192, 0.3369, 0.4398,  ..., 0.8528, 0.3716, 0.2348],\n        [0.2730, 0.4278, 0.6589,  ..., 0.7106, 0.2968, 0.0330],\n        [0.7017, 0.8688, 0.9488,  ..., 0.0027, 0.2373, 0.6707],\n        [0.6083, 0.3034, 0.7345,  ..., 0.2399, 0.2169, 0.5537]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sequence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gueARUu0KFVm"
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Calculating self-attention score from **`Query`** and **`Key`** vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TwUclKXLN14",
    "outputId": "49cb0e0f-0e59-46d2-cf72-0b62873ec904",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:45:18.556990Z",
     "start_time": "2024-06-05T19:45:18.551291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[23.4544, 25.5804, 13.9753, 26.0646],\n        [20.3942, 26.7025, 21.0241, 18.7530],\n        [21.9645, 18.2919, 11.9148, 21.8382],\n        [32.4823,  7.8042, 15.8128, 18.4249]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.mm(queries, keys.T)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZugp0egQEPQ"
   },
   "source": [
    "## Step 3\n",
    "Divide the scores by 8 (This leads to having more stable gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gA1G9MmeQPuP",
    "outputId": "1f2fcf78-de48-4987-d17d-4c2a56e6a7c6",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:45:20.973217Z",
     "start_time": "2024-06-05T19:45:20.969441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.9318, 3.1976, 1.7469, 3.2581],\n        [2.5493, 3.3378, 2.6280, 2.3441],\n        [2.7456, 2.2865, 1.4893, 2.7298],\n        [4.0603, 0.9755, 1.9766, 2.3031]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores / 8\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-MZsAkWQWlp"
   },
   "source": [
    "## Step 4\n",
    "\n",
    "Pass the result through a softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_hpI-SNKqAA",
    "outputId": "88415710-2b74-4442-ba93-e1b47981f081",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:45:32.734267Z",
     "start_time": "2024-06-05T19:45:32.728560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2503, 0.3264, 0.0765, 0.3468],\n        [0.1962, 0.4317, 0.2123, 0.1598],\n        [0.3447, 0.2178, 0.0982, 0.3393],\n        [0.7447, 0.0341, 0.0927, 0.1285]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.nn.functional.softmax(scores, dim=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HskCcOiRQlam"
   },
   "source": [
    "## Step 5 & 6\n",
    "\n",
    "* Multiply each value vector by the softmax score\n",
    "* Sum up the weighted value vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aT3lZwy4TLK1",
    "outputId": "c0ca09e9-cce2-4ee0-c577-8e0dbfc013e5",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:45:43.738049Z",
     "start_time": "2024-06-05T19:45:43.734084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 4]), torch.Size([4, 7]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDsYOfEnHWVq",
    "outputId": "477a3083-d79c-41da-815c-694777f42879",
    "ExecuteTime": {
     "end_time": "2024-06-05T19:45:53.809153Z",
     "start_time": "2024-06-05T19:45:53.805039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0534, -1.8065,  1.7319,  2.0988,  4.5916, -1.1464, -4.1188],\n        [ 1.4203, -1.4870,  1.8724,  1.9259,  4.4651, -0.9034, -3.8619],\n        [ 1.2786, -1.9673,  1.4333,  2.3555,  4.2757, -1.0897, -3.9446],\n        [ 2.1994, -2.6760,  0.3326,  3.0990,  3.2727, -0.8717, -3.7979]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = scores @ values\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbCd2yNhzGv5"
   },
   "source": [
    "# Multi-headed attention\n",
    "\n",
    "**GOAL**:\n",
    "1. Expand the model’s ability to focus on different positions\n",
    "2. Provide the attention layer multiple “representation subspaces”\n",
    "\n",
    "**Attention with $N$ just means repeating self attention algorithm $N$ times and joining the results**\n",
    "\n",
    "\n",
    "![](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)\n",
    "\n",
    "**Multi-headed attention steps:**\n",
    "1. Same as self-attention calculation, just n different times with different weight matrices\n",
    "2. Condense the $N$ z metrices down into a single matrix by concatinating the matrices then multiply them by an additional weights matrix `WO`\n",
    "\n",
    "Now the output z metrix is fed to the FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99lM0p2IUd2k"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "  temp = query.bmm(key.transpose(1, 2))\n",
    "  scale = query.size(-1) ** 0.5\n",
    "  softmax = f.softmax(temp / scale, dim=-1)\n",
    "  return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sph09F-Qdlw1"
   },
   "source": [
    "## Now lets make attention head"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MfKQevaDdrQu",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:00:54.205713Z",
     "start_time": "2024-06-05T20:00:54.198356Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in, dim_q, dim_k):\n",
    "        super().__init__()\n",
    "        # Fill in the missing parts of the constructor to initialize the query, key, and value linear transformations\n",
    "        # dim_in is the input dimension, dim_q and dim_k are the output dimensions for the queries and keys/values respectively\n",
    "        # Example: dim_in 512, dim_q = dim_k 64 in the paper\n",
    "        self.query_linear = nn.Linear(dim_in, dim_q)\n",
    "        self.key_linear = nn.Linear(dim_in, dim_k)\n",
    "        self.value_linear = nn.Linear(dim_in, dim_k)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Implement the forward pass by calling scaled_dot_product_attention function\n",
    "        # You need to transform the query, key, and value using the linear transformations defined in __init__\n",
    "        # Fill in with the correct method calls\n",
    "        query_transformed = self.query_linear(query)\n",
    "        key_transformed = self.key_linear(key)\n",
    "        value_transformed = self.value_linear(value)\n",
    "        \n",
    "        return scaled_dot_product_attention(query_transformed, key_transformed, value_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig3CKeuseEOs"
   },
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L_kYFonCeD-y",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:06:55.862147Z",
     "start_time": "2024-06-05T20:06:55.857665Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class MultiHeadToAttention(nn.Module):\n",
    "    def __init__(self, number_of_heads, dim_in, dim_q, dim_k):\n",
    "        super().__init__()\n",
    "        # Initialize heads as multi-AttentionHead instances\n",
    "        # Initialize linear to combine the outputs of all heads into a single output\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(number_of_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(number_of_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor):\n",
    "        # Concatenate outputs from all heads and apply the final linear transformation\n",
    "        head_outputs = [head(query, key, value) for head in self.heads]\n",
    "        concatenated_outputs = torch.cat(head_outputs, dim=-1)\n",
    "        final_output = self.linear(concatenated_outputs)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk0KG1p0UeSV"
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "A way to account for the order of the words in the input sequence. A transformer adds a vector to each input embedding which helps it determine the position of each word. <br>\n",
    "**Goal** : preserving information about the order of tokens  \n",
    "\n",
    "Positional Encoding they can either be learned or fixed a priori.\n",
    "\n",
    "Proposed approach from original paper : describe a simple scheme for fixed positional encodings based on sine and cosine functions\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*C3a9RL6-SFC6fW8NGpJg5A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q_AGKKflld67",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:12:56.791726Z",
     "start_time": "2024-06-05T20:12:56.788228Z"
    }
   },
   "outputs": [],
   "source": [
    "def position_encoding(seq_len, dim_model, device):\n",
    "    # Define the position tensor 'pos' with dimensions appropriate for sequence length\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "    # Define the dimension tensor 'dim' suitable for the model dimensions\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).unsqueeze(0)\n",
    "\n",
    "    # Calculate the phase using the position and dimension tensors\n",
    "    phase = pos / (10000 ** (2 * (dim // 2) / dim_model))\n",
    "\n",
    "    # Return the sinusoidal position encoding\n",
    "    pos_encoding = torch.zeros((seq_len, dim_model), device=device)\n",
    "    pos_encoding[:, 0::2] = torch.sin(phase[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = torch.cos(phase[:, 1::2])\n",
    "    return  pos_encoding # Complete this line to select sin or cos based on even/odd index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB6Z4v4dnlYk"
   },
   "source": [
    "## Encoder Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "19MLi_k0nuZr",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:31:35.980596Z",
     "start_time": "2024-06-05T20:31:35.976945Z"
    }
   },
   "outputs": [],
   "source": [
    "def feed_forward(dim_input = 512, dim_feedforward = 2048):\n",
    "  return nn.Sequential(nn.Linear(dim_input, dim_feedforward),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(dim_feedforward, dim_input)\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctJcODqZoNjm"
   },
   "source": [
    "## Encoder Residual\n",
    "\n",
    "From the original paper the author implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Z3jxUDJxoJrh",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:31:36.805952Z",
     "start_time": "2024-06-05T20:31:36.802578Z"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "  def __init__(self, sublayer, dimension, dropout = 0.1):\n",
    "    super().__init__()\n",
    "    self.sublayer = sublayer\n",
    "    self.norm = nn.LayerNorm(dimension)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, *tensors):\n",
    "    return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbO7zQf6Z6oC"
   },
   "source": [
    "## Putting all together on encoder side\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_IWAB7fpdnX"
   },
   "source": [
    "## Putting the Encoder layer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6mclItDepir9",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:18:35.994395Z",
     "start_time": "2024-06-05T20:18:35.990492Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model=512, num_heads=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Define dimensions for query and key based on model dimension and number of heads\n",
    "        dim_q = dim_k = dim_model // num_heads\n",
    "\n",
    "        # Initialize the MultiHeadAttention component with a residual connection and dropout\n",
    "        self.attention = Residual(\n",
    "            MultiHeadToAttention(num_heads, dim_model, dim_q, dim_k), dim_model, dropout\n",
    "        )\n",
    "\n",
    "        # Initialize the feedforward component with a residual connection and dropout\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward), dim_model, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Apply the attention mechanism to the source data\n",
    "        src = self.attention(src, src, src)\n",
    "\n",
    "        # Apply the feedforward network to the output of the attention mechanism\n",
    "        src = self.feed_forward(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfQ17Qy2pubd"
   },
   "source": [
    "## Putting together transfomer Encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kwXf_WW-puz_",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:20:29.671050Z",
     "start_time": "2024-06-05T20:20:29.667104Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers=12, dim_model=512, num_heads=4, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Initialize a list of TransformerEncoderLayer instances\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Retrieve the sequence length and dimension from the source tensor\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "\n",
    "        # Add position encoding to the source tensor\n",
    "        src += position_encoding(seq_len, self.dim_model, src.device)\n",
    "\n",
    "        # Process each layer in the transformer encoder\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9iOQWX8aQn9"
   },
   "source": [
    "# The Decoder Side\n",
    "\n",
    "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder.\n",
    "\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehLixHKossPK"
   },
   "source": [
    "## Decoder layer\n",
    "\n",
    "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n",
    "\n",
    "**Task**: implement the decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "D8Kpvt0Gsxiq",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:25:29.449173Z",
     "start_time": "2024-06-05T20:25:29.444380Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init dim_q and dim_k as in the encoder\n",
    "        dim_q = dim_k = dim_model // num_heads\n",
    "        # Initialize the first self-attention layer with a residual connection\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadToAttention(num_heads, dim_model, dim_q, dim_k), dim_model, dropout\n",
    "        )\n",
    "\n",
    "        # Initialize the second attention layer for interaction with the encoder output\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadToAttention(num_heads, dim_model, dim_q, dim_k), dim_model, dropout\n",
    "        )\n",
    "\n",
    "        # Initialize the feed-forward network\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward), dim_model, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        # Self-attention mechanism\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "\n",
    "        # Cross-attention mechanism where the decoder attends to the encoder's output\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "\n",
    "        # Pass through the feed-forward network\n",
    "        tgt = self.feed_forward(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g8J94rHsusT"
   },
   "source": [
    "## Full Transfomer Decoder\n",
    "\n",
    "**Task**: implement the transfomer decoder part class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EnXSvVUUs4sy",
    "ExecuteTime": {
     "end_time": "2024-06-05T20:27:51.936920Z",
     "start_time": "2024-06-05T20:27:51.932689Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Initialize laters from TransformerDecoderLayer instances\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        # Define the output linear transformation\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        # Calculate sequence length and dimension from the target tensor\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "\n",
    "        # Add position encoding to the target tensor\n",
    "        tgt += position_encoding(seq_len, dimension, tgt.device)\n",
    "\n",
    "        # Process each layer in the transformer decoder\n",
    "        # Code here\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        output = self.linear(tgt)\n",
    "\n",
    "        # Apply a softmax to the output of the final linear layer\n",
    "        output = F.softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-6Uav2htlEF"
   },
   "source": [
    "## Full Transfomer model\n",
    "\n",
    "**Task**:\n",
    "1. Assembly a full transfomer (Encoder + Decoder)\n",
    "2. Implement the Transfomer training loop\n",
    "3. Using dataset of your choice train the transformer just for one epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, dim_model, num_heads, dim_feedforward, input_vocab_size, target_vocab_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(num_layers=num_encoder_layers, dim_model=dim_model, num_heads=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.decoder = TransformerDecoder(num_layers=num_decoder_layers, dim_model=dim_model, num_heads=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, dim_model)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, dim_model)\n",
    "        self.linear = nn.Linear(dim_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.input_embedding(src)\n",
    "        tgt_embedded = self.target_embedding(tgt)\n",
    "\n",
    "        encoder_output = self.encoder(src_embedded)\n",
    "        decoder_output = self.decoder(tgt_embedded, encoder_output)\n",
    "\n",
    "        output = self.linear(decoder_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T20:29:54.841908Z",
     "start_time": "2024-06-05T20:29:54.837245Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_loss(predictions, targets, pad_index):\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "    loss = loss_fn(predictions.view(-1, predictions.size(-1)), targets.view(-1))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T20:30:20.924861Z",
     "start_time": "2024-06-05T20:30:20.920906Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, pad_index):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(src, tgt[:, :-1] if tgt.dim() > 1 else tgt)\n",
    "        loss = calculate_loss(predictions, tgt[:, 1:] if tgt.dim() > 1 else tgt, pad_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T20:32:45.115147Z",
     "start_time": "2024-06-05T20:32:45.111850Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, optimizer, pad_index, num_epochs=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_epoch(model, train_loader, optimizer, pad_index)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T20:30:33.221935Z",
     "start_time": "2024-06-05T20:30:33.218618Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 29\u001B[0m\n\u001B[0;32m     26\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m [(torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m1\u001B[39m, input_vocab_size, (\u001B[38;5;241m10\u001B[39m,)), torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m1\u001B[39m, target_vocab_size, (\u001B[38;5;241m10\u001B[39m,))) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m)]\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Обучение модели\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m \u001B[43mtrain_transformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[24], line 3\u001B[0m, in \u001B[0;36mtrain_transformer\u001B[1;34m(model, train_loader, optimizer, pad_index, num_epochs)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_transformer\u001B[39m(model, train_loader, optimizer, pad_index, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m----> 3\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[29], line 6\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, dataloader, optimizer, pad_index)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m src, tgt \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m      5\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m----> 6\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     loss \u001B[38;5;241m=\u001B[39m calculate_loss(predictions, tgt[:, \u001B[38;5;241m1\u001B[39m:] \u001B[38;5;28;01mif\u001B[39;00m tgt\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m tgt, pad_index)\n\u001B[0;32m      8\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mE:\\GitHub\\LLM-Innopolis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\GitHub\\LLM-Innopolis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[21], line 14\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, src, tgt)\u001B[0m\n\u001B[0;32m     11\u001B[0m src_embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_embedding(src)\n\u001B[0;32m     12\u001B[0m tgt_embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_embedding(tgt)\n\u001B[1;32m---> 14\u001B[0m encoder_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_embedded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m decoder_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(tgt_embedded, encoder_output)\n\u001B[0;32m     17\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(decoder_output)\n",
      "File \u001B[1;32mE:\\GitHub\\LLM-Innopolis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\GitHub\\LLM-Innopolis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[18], line 13\u001B[0m, in \u001B[0;36mTransformerEncoder.forward\u001B[1;34m(self, src)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, src):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Retrieve the sequence length and dimension from the source tensor\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m     seq_len, dimension \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m), \u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# Add position encoding to the source tensor\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     src \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m position_encoding(seq_len, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim_model, src\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mIndexError\u001B[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Параметры модели\n",
    "    num_encoder_layers = 6\n",
    "    num_decoder_layers = 6\n",
    "    dim_model = 512\n",
    "    num_heads = 8\n",
    "    dim_feedforward = 2048\n",
    "    input_vocab_size = 10000\n",
    "    target_vocab_size = 10000\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Создание модели\n",
    "    model = Transformer(\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_model=dim_model,\n",
    "        num_heads=num_heads,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Пример оптимизатора и загрузчика данных (необходимо заменить на реальные данные)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_loader = [(torch.randint(1, input_vocab_size, (10,)), torch.randint(1, target_vocab_size, (10,))) for _ in range(100)]\n",
    "\n",
    "    # Обучение модели\n",
    "    train_transformer(model, train_loader, optimizer, pad_index=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T20:43:33.497604Z",
     "start_time": "2024-06-05T20:43:33.200414Z"
    }
   },
   "execution_count": 32
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
