{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "lwNHyfGYi2-p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create the rotation matrix for RoPE embeddings\n",
        "def generate_rotation_matrix(seq_length, embed_dim):\n",
        "    \"\"\"\n",
        "    Generates a rotation matrix for RoPE embeddings.\n",
        "\n",
        "    Args:\n",
        "        seq_length (int): The length of the sequence.\n",
        "        embed_dim (int): The dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The rotation matrix.\n",
        "    \"\"\"\n",
        "    # Ensure the embedding dimension is even\n",
        "    assert embed_dim % 2 == 0, \"Embedding dimension must be even.\"\n",
        "\n",
        "    # Calculate the position index\n",
        "    position = np.arange(seq_length)[:, np.newaxis]  # Shape: (seq_length, 1)\n",
        "    dimension = np.arange(embed_dim // 2)[np.newaxis, :]  # Shape: (1, embed_dim // 2)\n",
        "\n",
        "    # Calculate theta using the exponential decay formula\n",
        "    theta = 1.0 / (10000 ** (2 * dimension / embed_dim))  # Shape: (1, embed_dim // 2)\n",
        "\n",
        "    # Calculate the sine and cosine components\n",
        "    angle_rates = position * theta  # Shape: (seq_length, embed_dim // 2)\n",
        "    cos = np.cos(angle_rates)\n",
        "    sin = np.sin(angle_rates)\n",
        "\n",
        "    # Combine the cosine and sine components into a rotation matrix\n",
        "    rotation_matrix = np.zeros((seq_length, embed_dim, embed_dim))\n",
        "    for i in range(seq_length):\n",
        "        cos_i = cos[i]\n",
        "        sin_i = sin[i]\n",
        "        diag_cos = np.diag(cos_i)\n",
        "        diag_sin = np.diag(sin_i)\n",
        "        upper = np.concatenate([diag_cos, -diag_sin], axis=1)\n",
        "        lower = np.concatenate([diag_sin, diag_cos], axis=1)\n",
        "        rotation_matrix[i] = np.concatenate([upper, lower], axis=0)\n",
        "\n",
        "    return torch.tensor(rotation_matrix, dtype=torch.float32)\n",
        "\n",
        "# Example usage\n",
        "seq_length = 10\n",
        "embed_dim = 64\n",
        "rotation_matrix = generate_rotation_matrix(seq_length, embed_dim)\n",
        "print(rotation_matrix.shape)  # Should be (seq_length, embed_dim, embed_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gck72ZxU_5xL",
        "outputId": "440cada8-5fa1-4719-85b0-f03e1c522395"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, layer_dims, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(layer_dims))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, tensor_input):\n",
        "        # Calculate the root mean square (RMS) of the tensor values\n",
        "        rms = torch.sqrt(torch.mean(tensor_input ** 2, dim=-1, keepdim=True) + self.epsilon)\n",
        "\n",
        "        # Normalize the input tensor\n",
        "        tensor_normalized = tensor_input / rms\n",
        "\n",
        "        # Scale the normalized tensor by the learnable parameter gamma\n",
        "        output = self.gamma * tensor_normalized\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "tensor_input = torch.randn(10, 64)  # Example input with batch size 10 and feature dimension 64\n",
        "layer_dims = 64  # Example feature dimension\n",
        "rms_norm = RMSNorm(layer_dims)\n",
        "output = rms_norm(tensor_input)\n",
        "print(output.shape)  # Should be (10, 64)\n"
      ],
      "metadata": {
        "id": "0NVsho2LAFXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHeadWithRoPE(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, seq_length, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Initialize linear layers for queries, keys, and values\n",
        "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # Generate the rotation matrix for RoPE embeddings\n",
        "        self.rotation_matrix = generate_rotation_matrix(seq_length, embed_dim)\n",
        "\n",
        "    def forward(self, data_input, attention_mask=None, return_attention=False):\n",
        "        batch_size, seq_len, embed_dim = data_input.shape\n",
        "\n",
        "        # Transform the input data into queries, keys, and values\n",
        "        queries = self.query(data_input)\n",
        "        keys = self.key(data_input)\n",
        "        values = self.value(data_input)\n",
        "\n",
        "        # Apply RoPE rotation to queries and keys\n",
        "        queries = self.apply_rope(queries)\n",
        "        keys = self.apply_rope(keys)\n",
        "\n",
        "        # Reshape queries, keys, values for multi-head attention\n",
        "        queries = queries.view(batch_size, seq_len, self.num_heads, embed_dim // self.num_heads).transpose(1, 2)\n",
        "        keys = keys.view(batch_size, seq_len, self.num_heads, embed_dim // self.num_heads).transpose(1, 2)\n",
        "        values = values.view(batch_size, seq_len, self.num_heads, embed_dim // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (embed_dim // self.num_heads) ** 0.5\n",
        "        if attention_mask is not None:\n",
        "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, values)\n",
        "\n",
        "        # Reshape back to the original dimensions\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        if return_attention:\n",
        "            return attention_output, attention_weights\n",
        "        else:\n",
        "            return attention_output\n",
        "\n",
        "    def apply_rope(self, tensor):\n",
        "        batch_size, seq_len, embed_dim = tensor.shape\n",
        "        rope_matrix = self.rotation_matrix[:seq_len, :, :].to(tensor.device)\n",
        "        tensor = tensor.view(batch_size, seq_len, self.num_heads, embed_dim // self.num_heads)\n",
        "\n",
        "        tensor_rotated = torch.einsum('blhd,lhe->blhd', tensor, rope_matrix)\n",
        "        return tensor_rotated.view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Example usage\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "seq_length = 10\n",
        "data_input = torch.randn(2, seq_length, embed_dim)  # Example input with batch size 2\n",
        "\n",
        "attention_layer = AttentionHeadWithRoPE(embed_dim, num_heads, seq_length)\n",
        "output = attention_layer(data_input)\n",
        "print(output.shape)  # Should be (2, seq_length, embed_dim)\n"
      ],
      "metadata": {
        "id": "SaPweizcASCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, seq_length, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.seq_length = seq_length\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
        "\n",
        "        # Initialize multiple attention heads\n",
        "        self.attention_heads = nn.ModuleList(\n",
        "            [AttentionHeadWithRoPE(self.head_dim, 1, seq_length) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "        # Define an output linear transformation\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # Add dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, data_input, attention_mask=None):\n",
        "        # Compute the output for each attention head\n",
        "        head_outputs = [\n",
        "            head(data_input, attention_mask) for head in self.attention_heads\n",
        "        ]\n",
        "\n",
        "        # Concatenate them along the last dimension\n",
        "        concatenated_output = torch.cat(head_outputs, dim=-1)\n",
        "\n",
        "        # Apply the output linear transformation and dropout\n",
        "        output = self.output_linear(concatenated_output)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "seq_length = 10\n",
        "data_input = torch.randn(2, seq_length, embed_dim)  # Example input with batch size 2\n",
        "\n",
        "multihead_attention = MultiHeadAttention(embed_dim, num_heads, seq_length)\n",
        "output = multihead_attention(data_input)\n",
        "print(output.shape)  # Should be (2, seq_length, embed_dim)\n"
      ],
      "metadata": {
        "id": "HYn1Jep1AbYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwishGLU(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.linear_gate = nn.Linear(size, size)\n",
        "        self.linear = nn.Linear(size, size)\n",
        "        self.beta = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, data_input):\n",
        "        # Apply the linear transformations\n",
        "        gate = self.linear_gate(data_input)\n",
        "        x = self.linear(data_input)\n",
        "\n",
        "        # Apply Swish activation\n",
        "        swish_activation = gate * torch.sigmoid(self.beta * gate)\n",
        "\n",
        "        # Gated Linear Unit mechanism\n",
        "        output = x * swish_activation\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "input_size = 64\n",
        "data_input = torch.randn(2, input_size)  # Example input with batch size 2\n",
        "\n",
        "swish_glu = SwishGLU(input_size)\n",
        "output = swish_glu(data_input)\n",
        "print(output.shape)  # Should be (2, input_size)\n"
      ],
      "metadata": {
        "id": "Snou_PDwAkia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLamaBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, seq_length, feedforward_dim, dropout=0.1, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.norm1 = RMSNorm(embed_dim, epsilon)\n",
        "        self.norm2 = RMSNorm(embed_dim, epsilon)\n",
        "\n",
        "        self.attention_head = AttentionHeadWithRoPE(embed_dim, num_heads, seq_length)\n",
        "\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, feedforward_dim),\n",
        "            SwishGLU(feedforward_dim),\n",
        "            nn.Linear(feedforward_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, data_input, attention_mask=None):\n",
        "        # Apply RMS normalization and attention with residual connection\n",
        "        normalized_input = self.norm1(data_input)\n",
        "        attention_output = self.attention_head(normalized_input, attention_mask)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        data_input = data_input + attention_output\n",
        "\n",
        "        # Apply RMS normalization and feedforward network with residual connection\n",
        "        normalized_input = self.norm2(data_input)\n",
        "        feedforward_output = self.feedforward(normalized_input)\n",
        "        feedforward_output = self.dropout(feedforward_output)\n",
        "        data_input = data_input + feedforward_output\n",
        "\n",
        "        return data_input\n",
        "\n",
        "# Example usage\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "seq_length = 10\n",
        "feedforward_dim = 256\n",
        "data_input = torch.randn(2, seq_length, embed_dim)  # Example input with batch size 2\n",
        "\n",
        "llama_block = LLamaBlock(embed_dim, num_heads, seq_length, feedforward_dim)\n",
        "output = llama_block(data_input)\n",
        "print(output.shape)  # Should be (2, seq_length, embed_dim)\n"
      ],
      "metadata": {
        "id": "xQl4TMthAsbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLama(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, seq_length, feedforward_dim, num_blocks, dropout=0.1, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the embedding layer for the input tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Construct a sequence of transformer blocks using OrderedDict\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            OrderedDict([\n",
        "                (f\"block_{i}\", LLamaBlock(embed_dim, num_heads, seq_length, feedforward_dim, dropout, epsilon))\n",
        "                for i in range(num_blocks)\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        # Define the final linear transformation layers\n",
        "        self.final_linear1 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.swish_glu = SwishGLU(embed_dim)\n",
        "        self.final_linear2 = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Optionally print the total number of parameters in the model\n",
        "        print(\"Model parameters:\", sum([p.numel() for p in self.parameters()]))\n",
        "\n",
        "    def forward(self, input_ids, target_ids=None):\n",
        "        # Embed the input_ids\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        # Process them through the LLama_blocks\n",
        "        transformer_output = self.transformer_blocks(embeddings)\n",
        "\n",
        "        # Apply the final linear transformations\n",
        "        output = self.final_linear1(transformer_output)\n",
        "        output = self.swish_glu(output)\n",
        "        logits = self.final_linear2(output)\n",
        "\n",
        "        loss = None\n",
        "        if target_ids is not None:\n",
        "            # Compute cross-entropy loss\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            # Shift logits and targets to align predictions with the next token in the sequence\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            target_ids = target_ids.view(-1)\n",
        "            loss = loss_fn(logits, target_ids)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# Example usage\n",
        "vocab_size = 30522  # Example vocabulary size for a tokenizer\n",
        "embed_dim = 768\n",
        "num_heads = 12\n",
        "seq_length = 512\n",
        "feedforward_dim = 3072\n",
        "num_blocks = 12\n",
        "data_input = torch.randint(0, vocab_size, (2, seq_length))  # Example input with batch size 2\n",
        "\n",
        "llama_model = LLama(vocab_size, embed_dim, num_heads, seq_length, feedforward_dim, num_blocks)\n",
        "logits, loss = llama_model(data_input, data_input)\n",
        "print(logits.shape)  # Should be (2*seq_length, vocab_size)\n",
        "if loss is not None:\n",
        "    print(loss.item())\n"
      ],
      "metadata": {
        "id": "Sxb4zYNnA6Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Define the necessary components for the LLama model, RMSNorm, AttentionHeadWithRoPE, SwishGLU, and LLamaBlock here\n",
        "\n",
        "# Data preprocessing function\n",
        "def prepare_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    unique_chars = sorted(set(text))\n",
        "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "    index_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
        "    data_indices = [char_to_index[char] for char in text]\n",
        "    data_tensor = torch.tensor(data_indices, dtype=torch.long)\n",
        "    vocabulary_size = len(unique_chars)\n",
        "    return data_tensor, vocabulary_size, char_to_index, index_to_char\n",
        "\n",
        "# Function to create training batches\n",
        "def batch_generator(data, split, batch_size, seq_len, params):\n",
        "    total_len = len(data)\n",
        "    train_len = int(total_len * 0.8)\n",
        "    val_len = int(total_len * 0.1)\n",
        "    train_data = data[:train_len]\n",
        "    val_data = data[train_len:train_len + val_len]\n",
        "    test_data = data[train_len + val_len:]\n",
        "\n",
        "    if split == 'train':\n",
        "        split_data = train_data\n",
        "    elif split == 'validation':\n",
        "        split_data = val_data\n",
        "    elif split == 'test':\n",
        "        split_data = test_data\n",
        "    else:\n",
        "        raise ValueError(\"split must be 'train', 'validation', or 'test'\")\n",
        "\n",
        "    total_batches = len(split_data) // (batch_size * seq_len)\n",
        "    input_sequences = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
        "    target_sequences = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
        "\n",
        "    for _ in range(total_batches):\n",
        "        start_indices = torch.randint(0, len(split_data) - seq_len, (batch_size,))\n",
        "        for i, start_idx in enumerate(start_indices):\n",
        "            input_sequences[i] = split_data[start_idx:start_idx + seq_len]\n",
        "            target_sequences[i] = split_data[start_idx + 1:start_idx + seq_len + 1]\n",
        "\n",
        "        yield input_sequences, target_sequences\n",
        "\n",
        "# Function to evaluate model loss\n",
        "@torch.no_grad()\n",
        "def eval_loss(model, dataset, params):\n",
        "    results = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"validation\"]:\n",
        "        batch_losses = []\n",
        "        for _ in range(10):\n",
        "            input_batch, target_batch = next(batch_generator(dataset, split, params['training_batch'], params['sequence_length'], params))\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "            _, batch_loss = model(input_batch, target_batch)\n",
        "            batch_losses.append(batch_loss.item())\n",
        "        results[split] = np.mean(batch_losses)\n",
        "    model.train()\n",
        "    return results\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, optimizer, dataset, params, scheduler=None):\n",
        "    all_losses = []\n",
        "    start = time.time()\n",
        "    for epoch in range(params['training_epochs']):\n",
        "        optimizer.zero_grad()\n",
        "        input_batch, target_batch = next(batch_generator(dataset, 'train', params['training_batch'], params['sequence_length'], params))\n",
        "\n",
        "        input_batch = input_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        _, batch_loss = model(input_batch, target_batch)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % params['logging_frequency'] == 0:\n",
        "            time_elapsed = time.time() - start\n",
        "            evaluation_result = eval_loss(model, dataset, params)\n",
        "            all_losses.append(evaluation_result)\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1}/{params['training_epochs']} | \"\n",
        "                f\"Validation Loss: {evaluation_result['validation']:.4f} | \"\n",
        "                f\"Time: {time_elapsed:.2f}s\"\n",
        "            )\n",
        "            start = time.time()\n",
        "            if scheduler:\n",
        "                print(\"lr: \", scheduler.get_last_lr())\n",
        "\n",
        "    print(\"Final Validation Loss: \", all_losses[-1]['validation'])\n",
        "    return pd.DataFrame(all_losses).plot()\n",
        "\n",
        "# Load and process the dataset\n",
        "data_path = 'path_to_text_file.txt'\n",
        "dataset, vocab_size, char_to_index, index_to_char = prepare_data(data_path)\n",
        "\n",
        "# Configuration for the language model\n",
        "MODEL_PARAMS = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': 768,\n",
        "    'num_heads': 12,\n",
        "    'seq_length': 512,\n",
        "    'feedforward_dim': 3072,\n",
        "    'num_blocks': 12,\n",
        "    'dropout': 0.1,\n",
        "    'epsilon': 1e-8,\n",
        "    'training_batch': 32,\n",
        "    'sequence_length': 50,\n",
        "    'training_epochs': 10,\n",
        "    'logging_frequency': 1\n",
        "}\n",
        "\n",
        "# Check GPU availability and set device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Create the language model\n",
        "model = LLama(MODEL_PARAMS).to(device)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Start training\n",
        "train_model(model, optimizer, dataset, MODEL_PARAMS)\n"
      ],
      "metadata": {
        "id": "OGeOcmhVB34_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
